{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS/PC - Import splinter and set the cromedriver path\n",
    "# executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "# browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# MAC - Import splinter and set the cromedriver path\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Subreddits for Subscriber Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\"AZCardinals\", \"falcons\", \"ravens\", \"buffalobills\", \"panthers\",\"CHIBears\", \"bengals\", \"Browns\",\n",
    "              \"cowboys\", \"DenverBroncos\", \"detroitlions\", \"GreenBayPackers\", \"Texans\", \"Colts\", \"Jaguars\",\n",
    "              \"KansasCityChiefs\", \"miamidolphins\", \"minnesotavikings\", \"Patriots\", \"Saints\", \"NYGiants\", \"nyjets\",\n",
    "              \"oaklandraiders\", \"eagles\", \"steelers\", \"LosAngelesRams\", \"Chargers\", \"49ers\", \"Seahawks\", \n",
    "              \"buccaneers\", \"Tennesseetitans\", \"Redskins\"]\n",
    "\n",
    "subcounts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in subreddits:\n",
    "    time.sleep(1)\n",
    "    url = f\"https://www.reddit.com/r/{sub}\"\n",
    "    browser.visit(url)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    subscribers = int(float(soup.find(\"p\", class_=\"s1bd5ppi-10\").text.split(\"k\")[0])*1000)\n",
    "    dictionary = {\"Subreddit URL (https://www.reddit.com/r/)\":sub, \"Subs\":subscribers}\n",
    "    subcounts.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(subcounts)\n",
    "teams = pd.read_csv(\"Teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(df, teams, on=\"Subreddit URL (https://www.reddit.com/r/)\")\n",
    "combined = combined.drop([\"Subreddit URL (https://www.reddit.com/r/)\"], axis=1)\n",
    "combined.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"TeamsSubs.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Team Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_url = \"https://www.reddit.com/r/nfl/comments/9hflml/forbes_nfl_2018_team_valuations_most_valuable/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(values_url)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = tables[0]\n",
    "values_df.columns = [\"Team\", \"Value ($B)\", \"Revenue ($M)\", \"Operating Income ($M)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_combined = pd.merge(combined, values_df, on=\"Team\")\n",
    "values_combined.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_combined.to_csv(\"TeamsSubsValues.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping City Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = values_combined[\"Real City\"].tolist()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [item.replace(\", \", \"-\") for item in cities]\n",
    "cities = [item.replace(\"New York City\", \"New York\") for item in cities]\n",
    "cities = [item.replace(\" \", \"-\") for item in cities]\n",
    "cities = [item.replace(\"Nashville-Tennessee\", \"Nashville-Davidson-Tennessee\") for item in cities]\n",
    "cities = [item.replace(\"Washington-DC\", \"Washington-District-of-Columbia\") for item in cities]\n",
    "cities_urls = [item + \".html\" for item in cities]\n",
    "cities_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_combined['City URL'] = pd.Series(cities_urls)\n",
    "values_combined.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = []\n",
    "\n",
    "for city in cities_urls:\n",
    "    pop_url = f\"http://www.city-data.com/city/{city}\"\n",
    "    browser.visit(pop_url)\n",
    "    time.sleep(1)\n",
    "    pop_html = browser.html\n",
    "    pop_soup = bs(pop_html, \"html.parser\")\n",
    "    population = int(float(pop_soup.find(\"section\", class_=\"city-population\").text.split(\":\")[1].strip().replace(\",\",\"\")))\n",
    "    pop_dictionary = {\"City URL\":city, \"Population (2016)\":population}\n",
    "    populations.append(pop_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.DataFrame(populations)\n",
    "values_pop_df = pd.merge(values_combined, pop_df, on=\"City URL\")\n",
    "values_pop_df = final_df.drop([\"City URL\"], axis=1)\n",
    "values_pop_df.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_pop_df.to_csv(\"TeamsSubsValuesPops.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Team Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_url = \"https://en.wikipedia.org/wiki/National_Football_League\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_tables = pd.read_html(ages_url)\n",
    "ages_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_df = ages_tables[2]\n",
    "ages_df = ages_df[[\"Club[57]\", 'First season[59]']]\n",
    "ages_df.columns = [\"Team\", \"Founded\"]\n",
    "years = ages_df[\"Founded\"].tolist()\n",
    "years = [item[:4] for item in years]\n",
    "del years[16]\n",
    "years = years[:-1]\n",
    "years = [int(item) for item in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ages_df[\"Team\"].tolist()\n",
    "del teams[16]\n",
    "del teams[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanteams = []\n",
    "\n",
    "for foo in teams:\n",
    "    try:\n",
    "        team = foo.split(\"*\")[0]\n",
    "        cleanteams.append(team)\n",
    "    except:\n",
    "        cleanteams.append(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "currentyear = datetime.date.today().year\n",
    "\n",
    "teamages = []\n",
    "\n",
    "for x in years:\n",
    "    teamage = currentyear - x\n",
    "    teamages.append(teamage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_df = pd.DataFrame({\n",
    "    \"Team\": cleanteams,\n",
    "    \"Team Age\": teamages\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_df.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(values_pop_df, ages_df, on=\"Team\")\n",
    "final_df.head(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
